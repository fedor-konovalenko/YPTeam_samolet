{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Установка пакетов и импорт библиотек"
      ],
      "metadata": {
        "id": "eQ5nOsYuJNYf"
      },
      "id": "eQ5nOsYuJNYf"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics[detection]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-sJv4kwOv9XB",
        "outputId": "7942e592-40db-4985-e080-3bd143e100ed"
      },
      "id": "-sJv4kwOv9XB",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmetrics[detection]\n",
            "  Downloading torchmetrics-1.2.0-py3-none-any.whl (805 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m805.2/805.2 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics[detection]) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics[detection]) (2.0.1+cu118)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics[detection])\n",
            "  Downloading lightning_utilities-0.9.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: pycocotools>2.0.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics[detection]) (2.0.7)\n",
            "Requirement already satisfied: torchvision>=0.8 in /usr/local/lib/python3.10/dist-packages (from torchmetrics[detection]) (0.15.2+cu118)\n",
            "Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics[detection]) (23.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics[detection]) (4.5.0)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools>2.0.0->torchmetrics[detection]) (3.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics[detection]) (3.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics[detection]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics[detection]) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics[detection]) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics[detection]) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics[detection]) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics[detection]) (16.0.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.8->torchmetrics[detection]) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.8->torchmetrics[detection]) (9.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (2.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->torchmetrics[detection]) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.8->torchmetrics[detection]) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.8->torchmetrics[detection]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.8->torchmetrics[detection]) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.8->torchmetrics[detection]) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->torchmetrics[detection]) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (1.16.0)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.9.0 torchmetrics-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a023ba37",
        "outputId": "06a46b13-85d9-4f0c-baee-619272c79b74"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<contextlib.ExitStack at 0x7d07b81bdea0>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from torchmetrics.detection import MeanAveragePrecision\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from pycocotools.coco import COCO\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torchvision import datasets, models\n",
        "import torchvision.transforms.v2 as transforms\n",
        "\n",
        "from torchvision.utils import draw_bounding_boxes\n",
        "from torchvision.io import read_image, ImageReadMode\n",
        "\n",
        "from torchvision.models.detection.ssd import SSDClassificationHead\n",
        "from torchvision.models.detection import _utils\n",
        "from torchvision.models.detection import SSD300_VGG16_Weights\n",
        "from torch import nn\n",
        "\n",
        "plt.ion()"
      ],
      "id": "a023ba37"
    },
    {
      "cell_type": "code",
      "source": [
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yb6WIgigDdbT",
        "outputId": "018a3896-dd4d-4383-9bc9-f4d514b82500"
      },
      "id": "Yb6WIgigDdbT",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Глобальные переменные и пути к данным"
      ],
      "metadata": {
        "id": "c9_TxM_qJhEZ"
      },
      "id": "c9_TxM_qJhEZ"
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_SIZE = .75\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "dataDir = 'drive/MyDrive/Colab Notebooks/samolet/data/train/images/'\n",
        "\n",
        "modelDir = 'drive/MyDrive/Colab Notebooks/samolet/data/train/model/'\n",
        "dataType = 'default'\n",
        "annFile = '{}annotations/instances_{}.json'.format(dataDir,dataType)\n",
        "ann_path = 'drive/MyDrive/Colab Notebooks/samolet/data/train/images/annotations/instances_default.json'"
      ],
      "metadata": {
        "id": "IXWejj7y60zi"
      },
      "id": "IXWejj7y60zi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GL8GBnmvfFfI",
        "outputId": "50697fec-1206-44f5-914d-d125e98c8453"
      },
      "id": "GL8GBnmvfFfI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Загрузка аннотаций из Coco датасета"
      ],
      "metadata": {
        "id": "xQWTpIlRJnpW"
      },
      "id": "xQWTpIlRJnpW"
    },
    {
      "cell_type": "code",
      "source": [
        "coco = COCO(annFile)\n",
        "cats = coco.loadCats(coco.getCatIds())\n",
        "nms=[cat['name'] for cat in cats]\n",
        "print('COCO categories: \\n{}\\n'.format(' '.join(nms)))\n",
        "\n",
        "nms = set([cat['supercategory'] for cat in cats])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJlOOGjLpY93",
        "outputId": "18c5211e-0d95-45f5-9010-c72828a84597"
      },
      "id": "WJlOOGjLpY93",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=4.63s)\n",
            "creating index...\n",
            "index created!\n",
            "COCO categories: \n",
            "window empty filled\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Трансформации и аугментации\n",
        "\n"
      ],
      "metadata": {
        "id": "8uIViphRnn9Z"
      },
      "id": "8uIViphRnn9Z"
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(0.5),\n",
        "    transforms.ToTensor(),\n",
        "    #transforms.Resize((320,320)),\n",
        "    transforms.RandomPerspective(distortion_scale=0.1, p=0.5),\n",
        "    #transforms.RandomPerspective(distortion_scale=0.25, p=0.5),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    #transforms.RandomPhotometricDistort()\n",
        "    ])"
      ],
      "metadata": {
        "id": "GaGjaoG1S0jP"
      },
      "id": "GaGjaoG1S0jP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Класс датасета"
      ],
      "metadata": {
        "id": "V2MuKd02KFac"
      },
      "id": "V2MuKd02KFac"
    },
    {
      "cell_type": "code",
      "source": [
        "class BuildingsDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, coco, transform=None):\n",
        "        self.root = root\n",
        "        self.transforms = transform\n",
        "        self.coco = coco\n",
        "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Image ID\n",
        "        img_id = self.ids[index]\n",
        "        # List: get annotation id from coco\n",
        "        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
        "        # Dictionary: target coco_annotation file for an image\n",
        "        coco_annotation = self.coco.loadAnns(ann_ids)\n",
        "\n",
        "        # open the input image\n",
        "        path = self.coco.loadImgs(img_id)[0][\"file_name\"]\n",
        "        img = Image.open(os.path.join(self.root, path))\n",
        "\n",
        "        # number of objects in the image\n",
        "        num_objs = len(coco_annotation)\n",
        "\n",
        "        # Bounding boxes for objects\n",
        "        # In coco format, bbox = [xmin, ymin, width, height]\n",
        "        # In pytorch, the input should be [xmin, ymin, xmax, ymax]\n",
        "        boxes = []\n",
        "        for i in range(num_objs):\n",
        "            xmin = coco_annotation[i]['bbox'][0]\n",
        "            ymin = coco_annotation[i]['bbox'][1]\n",
        "            xmax = xmin + coco_annotation[i]['bbox'][2]\n",
        "            ymax = ymin + coco_annotation[i]['bbox'][3]\n",
        "            boxes.append([xmin, ymin, xmax, ymax])\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.as_tensor([ann[\"category_id\"] for ann in coco_annotation], dtype=torch.int64) - 1\n",
        "\n",
        "        # Annotation is in dictionary format\n",
        "        new_annotation = {}\n",
        "        new_annotation[\"boxes\"] = boxes\n",
        "        new_annotation[\"labels\"] = labels\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img = self.transforms(img)\n",
        "\n",
        "        return img, new_annotation\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "imgs = BuildingsDataset(root=dataDir, coco=coco, transform=transformer)"
      ],
      "metadata": {
        "id": "RkC-EfcvpFQH"
      },
      "id": "RkC-EfcvpFQH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Загрузчики"
      ],
      "metadata": {
        "id": "lC0vu1--w7iu"
      },
      "id": "lC0vu1--w7iu"
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = int(TRAIN_SIZE * len(imgs))\n",
        "valid_size = len(imgs) - train_size\n",
        "generator = torch.Generator().manual_seed(2109)\n",
        "train_dataset, valid_dataset = torch.utils.data.random_split(imgs, [train_size, valid_size], generator=generator)\n",
        "dataset_list = {'train': train_dataset, 'val': valid_dataset}\n",
        "dataset_sizes = {'train': train_size, 'val': valid_size}\n",
        "dataloaders = {x: torch.utils.data.DataLoader(dataset_list[x], batch_size=BATCH_SIZE,\n",
        "                                             shuffle=True, num_workers=0, collate_fn=lambda batch: tuple(zip(*batch))) for x in ['train', 'val']}\n"
      ],
      "metadata": {
        "id": "s0imsehBao4R"
      },
      "id": "s0imsehBao4R",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Загрузка и кастомизация модели\n",
        "**Подходят любые модели, которые понимают вот такой формат данных на входе:**\n",
        "\n",
        "```\n",
        "        boxes (FloatTensor[N, 4]): the ground-truth boxes in [x1, y1, x2, y2] format, with 0 <= x1 < x2 <= W and 0 <= y1 < y2 <= H.\n",
        "\n",
        "        labels (Int64Tensor[N]): the class label for each ground-truth box\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "ckbJz9dux4RB"
      },
      "id": "ckbJz9dux4RB"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Faster-RCNN**"
      ],
      "metadata": {
        "id": "NlZ8ARxgKg6v"
      },
      "id": "NlZ8ARxgKg6v"
    },
    {
      "cell_type": "code",
      "source": [
        "#from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "#def create_model(num_classes):\n",
        "#    model = models.detection.fasterrcnn_resnet50_fpn(weights=None, weights_backbone=None)\n",
        "\n",
        "#    model.roi_heads.box_predictor.cls_score = nn.Linear(1024,num_classes)\n",
        "\n",
        "#    return model"
      ],
      "metadata": {
        "id": "2hHliqDr7oDW"
      },
      "id": "2hHliqDr7oDW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SSD300-VGG16**"
      ],
      "metadata": {
        "id": "eI25rAqwKxNc"
      },
      "id": "eI25rAqwKxNc"
    },
    {
      "cell_type": "code",
      "source": [
        "#def create_model(num_classes=91, size=300):\n",
        "#    model = models.get_model(\"ssd300_vgg16\", weights=SSD300_VGG16_Weights.COCO_V1)\n",
        "#    in_channels = _utils.retrieve_out_channels(model.backbone, (size, size))\n",
        "#    num_anchors = model.anchor_generator.num_anchors_per_location()\n",
        "#    model.head.classification_head = SSDClassificationHead(\n",
        "#        in_channels=in_channels,\n",
        "#        num_anchors=num_anchors,\n",
        "#        num_classes=num_classes,\n",
        "#    )\n",
        "    # Image size for transforms.\n",
        "#    model.transform.min_size = (size,)\n",
        "#    model.transform.max_size = size\n",
        "#    return model\n",
        "\n",
        "#model = create_model(3, 300)\n",
        "#print(model)"
      ],
      "metadata": {
        "id": "4U5STj9-JL3T"
      },
      "id": "4U5STj9-JL3T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SSD-Mobilenet**"
      ],
      "metadata": {
        "id": "82Gjd3BtotVL"
      },
      "id": "82Gjd3BtotVL"
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.detection.ssdlite320_mobilenet_v3_large(num_classes=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OzkDK_jov8q",
        "outputId": "f6649587-09de-4c19-d13b-f6e0f8261696"
      },
      "id": "0OzkDK_jov8q",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v3_large-8738ca79.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v3_large-8738ca79.pth\n",
            "100%|██████████| 21.1M/21.1M [00:00<00:00, 51.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#inp = read_image(dataDir+'0000000154building.jpg').to(torch.float32)\n",
        "#predictions = model.eval()([inp, ])"
      ],
      "metadata": {
        "id": "cfdNNGYRrtpO"
      },
      "id": "cfdNNGYRrtpO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Вспомогательные классы для осреднения результата и сохранения лучшей модели"
      ],
      "metadata": {
        "id": "XmZYyB5OptC3"
      },
      "id": "XmZYyB5OptC3"
    },
    {
      "cell_type": "code",
      "source": [
        "class Averager:\n",
        "    def __init__(self):\n",
        "      self.current_total = 0.0\n",
        "      self.iterations = 0.0\n",
        "\n",
        "    def send(self, value):\n",
        "      self.current_total += value\n",
        "      self.iterations += 1\n",
        "\n",
        "    @property\n",
        "    def value(self):\n",
        "      if self.iterations == 0:\n",
        "        return 0\n",
        "      else:\n",
        "        return 1.0 * self.current_total / self.iterations\n",
        "\n",
        "    def reset(self):\n",
        "      self.current_total = 0.0\n",
        "      self.iterations = 0.0\n",
        "\n",
        "class SaveBestModel:\n",
        "    \"\"\"\n",
        "    Class to save the best model while training. If the current epoch's\n",
        "    validation loss is less than the previous least less, then save the\n",
        "    model state.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self, best_valid_loss=float('inf')\n",
        "    ):\n",
        "        self.best_valid_loss = best_valid_loss\n",
        "\n",
        "    def __call__(\n",
        "        self, current_valid_loss,\n",
        "        epoch, model, optimizer\n",
        "    ):\n",
        "      if current_valid_loss < self.best_valid_loss:\n",
        "        self.best_valid_loss = current_valid_loss\n",
        "        print(f\"\\nBest validation loss: {self.best_valid_loss}\")\n",
        "        print(f\"\\nSaving best model for epoch: {epoch+1}\\n\")\n",
        "        torch.save(model.state_dict(), modelDir + 'mobile_net.pth')"
      ],
      "metadata": {
        "id": "3fGpVajOTRph"
      },
      "id": "3fGpVajOTRph",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Функции для обучения и валидации"
      ],
      "metadata": {
        "id": "9a_8V7EqyiU_"
      },
      "id": "9a_8V7EqyiU_"
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_data_loader, model):\n",
        "  model.train()\n",
        "  print('Training')\n",
        "  global train_itr\n",
        "  global train_loss_list\n",
        "\n",
        "     # initialize tqdm progress bar\n",
        "  prog_bar = tqdm(train_data_loader, total=len(train_data_loader))\n",
        "\n",
        "  for i, data in enumerate(prog_bar):\n",
        "    optimizer.zero_grad()\n",
        "    images, targets = data\n",
        "\n",
        "    images = list(image.to(device) for image in images)\n",
        "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "    loss_dict = model(images, targets)\n",
        "    losses = sum(loss for loss in loss_dict.values())\n",
        "    loss_value = losses.item()\n",
        "    train_loss_list.append(loss_value)\n",
        "    train_loss_hist.send(loss_value)\n",
        "    losses.backward()\n",
        "    optimizer.step()\n",
        "    train_itr += 1\n",
        "\n",
        "        # update the loss value beside the progress bar for each iteration\n",
        "    prog_bar.set_description(desc=f\"Loss: {loss_value:.4f}\")\n",
        "  return train_loss_list"
      ],
      "metadata": {
        "id": "MtovNXR4kM__"
      },
      "id": "MtovNXR4kM__",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(valid_data_loader, model):\n",
        "  print('Validating')\n",
        "  global val_itr\n",
        "  global val_loss_list\n",
        "  #global map_list\n",
        "\n",
        "    # initialize tqdm progress bar\n",
        "  prog_bar = tqdm(valid_data_loader, total=len(valid_data_loader))\n",
        "\n",
        "  for i, data in enumerate(prog_bar):\n",
        "    images, targets = data\n",
        "\n",
        "    images = list(image.to(device) for image in images)\n",
        "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "    with torch.no_grad():\n",
        "      model.train()\n",
        "      loss_dict = model(images, targets)\n",
        "      metric = MeanAveragePrecision(iou_type=\"bbox\")\n",
        "      model.eval()\n",
        "      output = model(images, targets)\n",
        "      metric.update(output, targets)\n",
        "      map_50 = metric.compute()['map_50']\n",
        "\n",
        "    losses = sum(loss for loss in loss_dict.values())\n",
        "    loss_value = losses.item()\n",
        "    val_loss_list.append(loss_value)\n",
        "    #map_list.append(map_50)\n",
        "    #map_hist.send(map_50)\n",
        "    val_loss_hist.send(loss_value)\n",
        "    val_itr += 1\n",
        "    # update the loss value beside the progress bar for each iteration\n",
        "    prog_bar.set_description(desc=f\"Loss: {loss_value:.4f}---mAP50: {map_50:.4f}\")\n",
        "  return val_loss_list"
      ],
      "metadata": {
        "id": "GbApxMkmRjKn"
      },
      "id": "GbApxMkmRjKn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Обучение"
      ],
      "metadata": {
        "id": "-0onfzGAynK-"
      },
      "id": "-0onfzGAynK-"
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
        "#exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "model = model.to(device)\n",
        "train_loss_hist = Averager()\n",
        "val_loss_hist = Averager()\n",
        "#map_hist = Averager()\n",
        "train_itr = 1\n",
        "val_itr = 1\n",
        "# train and validation loss lists to store loss values of all...\n",
        "# ... iterations till ena and plot graphs for all iterations\n",
        "train_loss_list = []\n",
        "val_loss_list = []\n",
        "#map_list = []\n",
        "save_best_model = SaveBestModel()\n",
        "# name to save the trained model with\n",
        "NUM_EPOCHS = 50\n",
        "# start the training epochs\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  print(f\"\\nEPOCH {epoch+1} of {NUM_EPOCHS}\")\n",
        "  # reset the training and validation loss histories for the current epoch\n",
        "  train_loss_hist.reset()\n",
        "  val_loss_hist.reset()\n",
        "  # start timer and carry out training and validation\n",
        "  start = time.time()\n",
        "  train_loss = train(dataloaders['train'], model)\n",
        "  val_loss = validate(dataloaders['val'], model)\n",
        "  print(f\"Epoch #{epoch+1} train loss: {train_loss_hist.value:.3f}\")\n",
        "  print(f\"Epoch #{epoch+1} validation loss: {val_loss_hist.value:.3f}\")\n",
        "  end = time.time()\n",
        "  print(f\"Took {((end - start) / 60):.3f} minutes for epoch {epoch+1}\")\n",
        "  # save the best model till now if we have the least loss in the...\n",
        "  # ... current epoch\n",
        "  save_best_model(\n",
        "            val_loss_hist.value, epoch, model, optimizer\n",
        "        )"
      ],
      "metadata": {
        "id": "kgVNXRSpSSA4"
      },
      "id": "kgVNXRSpSSA4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Загрузка весов лучшей из моделей"
      ],
      "metadata": {
        "id": "xbzpFb3wy3qv"
      },
      "id": "xbzpFb3wy3qv"
    },
    {
      "cell_type": "code",
      "source": [
        "best_model_params_path = os.path.join(modelDir, 'mobile_net.pth')\n",
        "trained_model = model\n",
        "trained_model.load_state_dict(torch.load(best_model_params_path, map_location=torch.device('cpu')))"
      ],
      "metadata": {
        "id": "QmYRsZL12nii",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acb8b0c8-4936-4dc1-8980-b5a389745f09"
      },
      "id": "QmYRsZL12nii",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Функция для рисования картинок и границ объектов"
      ],
      "metadata": {
        "id": "YmdiQBx0y9xn"
      },
      "id": "YmdiQBx0y9xn"
    },
    {
      "cell_type": "code",
      "source": [
        "def draw(path, model):\n",
        "  image = read_image(path).to(torch.float32)\n",
        "  with torch.no_grad():\n",
        "    x = image.to(device)\n",
        "    predictions = model.eval()([x, ])\n",
        "  pred = predictions[0]\n",
        "  image = (255.0 * (image - image.min()) / (image.max() - image.min())).to(torch.uint8)\n",
        "  #pred_score = [f\"value: {score:.3f}\" for score in pred[\"scores\"]]\n",
        "  #pred_label = [f\"value: {label}\" for label in pred[\"labels\"]]\n",
        "  pred_boxes = pred[\"boxes\"].long()\n",
        "  output_image = draw_bounding_boxes(image, pred_boxes, fill=True, width=3)\n",
        "\n",
        "  plt.figure(figsize=(12, 12))\n",
        "  plt.imshow(output_image.permute(1, 2, 0))\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "PSMj3m75O1a1"
      },
      "id": "PSMj3m75O1a1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Проверка на тестовых изображениях"
      ],
      "metadata": {
        "id": "d7RU-l4ZzEy3"
      },
      "id": "d7RU-l4ZzEy3"
    },
    {
      "cell_type": "code",
      "source": [
        "testDir = 'drive/MyDrive/Colab Notebooks/samolet/data/test/'\n",
        "tests = []\n",
        "for filename in os.listdir(testDir):\n",
        "  tests.append(filename)\n",
        "\n",
        "for i in np.random.randint(0, len(tests), 3):\n",
        "  draw(testDir + tests[i], trained_model)"
      ],
      "metadata": {
        "id": "ZcgPBflaOQGz"
      },
      "id": "ZcgPBflaOQGz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D2d8hw8-3Jlg"
      },
      "id": "D2d8hw8-3Jlg",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}