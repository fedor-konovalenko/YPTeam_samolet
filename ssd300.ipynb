{"cells":[{"cell_type":"code","source":["!pip install torchmetrics[detection]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-sJv4kwOv9XB","executionInfo":{"status":"ok","timestamp":1695283323625,"user_tz":-180,"elapsed":7137,"user":{"displayName":"Fedor Konovalenko","userId":"16758494924099257201"}},"outputId":"68c142ec-8f21-41fe-ac8d-76676488f57b"},"id":"-sJv4kwOv9XB","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torchmetrics[detection]\n","  Downloading torchmetrics-1.1.2-py3-none-any.whl (764 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m764.8/764.8 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics[detection]) (1.23.5)\n","Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics[detection]) (2.0.1+cu118)\n","Collecting lightning-utilities>=0.8.0 (from torchmetrics[detection])\n","  Downloading lightning_utilities-0.9.0-py3-none-any.whl (23 kB)\n","Requirement already satisfied: torchvision>=0.8 in /usr/local/lib/python3.10/dist-packages (from torchmetrics[detection]) (0.15.2+cu118)\n","Requirement already satisfied: pycocotools>2.0.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics[detection]) (2.0.7)\n","Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics[detection]) (23.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics[detection]) (4.5.0)\n","Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools>2.0.0->torchmetrics[detection]) (3.7.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics[detection]) (3.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics[detection]) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics[detection]) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics[detection]) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics[detection]) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics[detection]) (3.27.4.1)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics[detection]) (16.0.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.8->torchmetrics[detection]) (2.31.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.8->torchmetrics[detection]) (9.4.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (1.1.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (0.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (4.42.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (1.4.5)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (2.8.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->torchmetrics[detection]) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.8->torchmetrics[detection]) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.8->torchmetrics[detection]) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.8->torchmetrics[detection]) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.8->torchmetrics[detection]) (2023.7.22)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->torchmetrics[detection]) (1.3.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (1.16.0)\n","Installing collected packages: lightning-utilities, torchmetrics\n","Successfully installed lightning-utilities-0.9.0 torchmetrics-1.1.2\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11331,"status":"ok","timestamp":1695283340128,"user":{"displayName":"Fedor Konovalenko","userId":"16758494924099257201"},"user_tz":-180},"id":"a023ba37","outputId":"a651f7a9-f4aa-4264-9d8f-d99cd8ed4dc4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<contextlib.ExitStack at 0x788f4a11fdf0>"]},"metadata":{},"execution_count":2}],"source":["from torchmetrics.detection import MeanAveragePrecision\n","import os\n","import pandas as pd\n","import numpy as np\n","from matplotlib import pyplot as plt\n","from pycocotools.coco import COCO\n","import time\n","from tqdm import tqdm\n","from PIL import Image\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","from torchvision import datasets, models\n","import torchvision.transforms.v2 as transforms\n","\n","from torchvision.utils import draw_bounding_boxes\n","from torchvision.io import read_image, ImageReadMode\n","\n","from torchvision.models.detection.ssd import SSDClassificationHead\n","from torchvision.models.detection import _utils\n","from torchvision.models.detection import SSD300_VGG16_Weights\n","\n","plt.ion()"],"id":"a023ba37"},{"cell_type":"code","source":["device = (\n","    \"cuda\"\n","    if torch.cuda.is_available()\n","    else \"mps\"\n","    if torch.backends.mps.is_available()\n","    else \"cpu\"\n",")\n","print(f\"Using {device} device\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yb6WIgigDdbT","executionInfo":{"status":"ok","timestamp":1695283344087,"user_tz":-180,"elapsed":241,"user":{"displayName":"Fedor Konovalenko","userId":"16758494924099257201"}},"outputId":"87877900-ca39-4e06-e7dd-4c04a76bd950"},"id":"Yb6WIgigDdbT","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Using cpu device\n"]}]},{"cell_type":"code","source":["TRAIN_SIZE = .75\n","BATCH_SIZE = 16\n","\n","dataDir = 'drive/MyDrive/Colab Notebooks/samolet/data/train/images/'\n","\n","modelDir = 'drive/MyDrive/Colab Notebooks/samolet/data/train/model/'\n","dataType = 'default'\n","annFile = '{}annotations/instances_{}.json'.format(dataDir,dataType)\n","ann_path = 'drive/MyDrive/Colab Notebooks/samolet/data/train/images/annotations/instances_default.json'"],"metadata":{"id":"IXWejj7y60zi","executionInfo":{"status":"ok","timestamp":1695283357104,"user_tz":-180,"elapsed":233,"user":{"displayName":"Fedor Konovalenko","userId":"16758494924099257201"}}},"id":"IXWejj7y60zi","execution_count":4,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GL8GBnmvfFfI","executionInfo":{"status":"ok","timestamp":1695283374536,"user_tz":-180,"elapsed":16502,"user":{"displayName":"Fedor Konovalenko","userId":"16758494924099257201"}},"outputId":"5f334d98-4524-4352-fda2-7eb5880b7e22"},"id":"GL8GBnmvfFfI","execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["coco = COCO(annFile)\n","cats = coco.loadCats(coco.getCatIds())\n","nms=[cat['name'] for cat in cats]\n","print('COCO categories: \\n{}\\n'.format(' '.join(nms)))\n","\n","nms = set([cat['supercategory'] for cat in cats])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WJlOOGjLpY93","executionInfo":{"status":"ok","timestamp":1695283383290,"user_tz":-180,"elapsed":6898,"user":{"displayName":"Fedor Konovalenko","userId":"16758494924099257201"}},"outputId":"fc038af5-cd22-44be-db19-4f997e1bd116"},"id":"WJlOOGjLpY93","execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["loading annotations into memory...\n","Done (t=6.63s)\n","creating index...\n","index created!\n","COCO categories: \n","window empty filled\n","\n"]}]},{"cell_type":"markdown","source":["## Загрузчики\n","❗❗❗ **Вот тут можно экспериментировать**\n","\n"],"metadata":{"id":"8uIViphRnn9Z"},"id":"8uIViphRnn9Z"},{"cell_type":"code","source":["transformer = transforms.Compose([\n","    transforms.RandomHorizontalFlip(0.5),\n","    transforms.ToTensor()\n","    #transforms.RandomPhotometricDistort()\n","    ])"],"metadata":{"id":"GaGjaoG1S0jP","executionInfo":{"status":"ok","timestamp":1695283584141,"user_tz":-180,"elapsed":243,"user":{"displayName":"Fedor Konovalenko","userId":"16758494924099257201"}}},"id":"GaGjaoG1S0jP","execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["## Загрузчики"],"metadata":{"id":"lC0vu1--w7iu"},"id":"lC0vu1--w7iu"},{"cell_type":"code","source":["class BuildingsDataset(torch.utils.data.Dataset):\n","    def __init__(self, root, coco, transform=None):\n","        self.root = root\n","        self.transforms = transform\n","        self.coco = coco\n","        self.ids = list(sorted(self.coco.imgs.keys()))\n","\n","    def __getitem__(self, index):\n","        # Image ID\n","        img_id = self.ids[index]\n","        # List: get annotation id from coco\n","        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n","        # Dictionary: target coco_annotation file for an image\n","        coco_annotation = self.coco.loadAnns(ann_ids)\n","\n","        # open the input image\n","        path = self.coco.loadImgs(img_id)[0][\"file_name\"]\n","        img = Image.open(os.path.join(self.root, path))\n","\n","        # number of objects in the image\n","        num_objs = len(coco_annotation)\n","\n","        # Bounding boxes for objects\n","        # In coco format, bbox = [xmin, ymin, width, height]\n","        # In pytorch, the input should be [xmin, ymin, xmax, ymax]\n","        boxes = []\n","        for i in range(num_objs):\n","            xmin = coco_annotation[i]['bbox'][0]\n","            ymin = coco_annotation[i]['bbox'][1]\n","            xmax = xmin + coco_annotation[i]['bbox'][2]\n","            ymax = ymin + coco_annotation[i]['bbox'][3]\n","            boxes.append([xmin, ymin, xmax, ymax])\n","        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n","        labels = torch.as_tensor([ann[\"category_id\"] for ann in coco_annotation], dtype=torch.int64) - 1\n","\n","        # Annotation is in dictionary format\n","        new_annotation = {}\n","        new_annotation[\"boxes\"] = boxes\n","        new_annotation[\"labels\"] = labels\n","\n","        if self.transforms is not None:\n","            img = self.transforms(img)\n","\n","        return img, new_annotation\n","\n","    def __len__(self):\n","        return len(self.ids)\n","\n","imgs = BuildingsDataset(root=dataDir, coco=coco, transform=transformer)"],"metadata":{"id":"RkC-EfcvpFQH","executionInfo":{"status":"ok","timestamp":1695283599960,"user_tz":-180,"elapsed":392,"user":{"displayName":"Fedor Konovalenko","userId":"16758494924099257201"}}},"id":"RkC-EfcvpFQH","execution_count":8,"outputs":[]},{"cell_type":"code","source":["train_size = int(TRAIN_SIZE * len(imgs))\n","valid_size = len(imgs) - train_size\n","generator = torch.Generator().manual_seed(2109)\n","train_dataset, valid_dataset = torch.utils.data.random_split(imgs, [train_size, valid_size], generator=generator)\n","dataset_list = {'train': train_dataset, 'val': valid_dataset}\n","dataset_sizes = {'train': train_size, 'val': valid_size}\n","dataloaders = {x: torch.utils.data.DataLoader(dataset_list[x], batch_size=BATCH_SIZE,\n","                                             shuffle=True, num_workers=0, collate_fn=lambda batch: tuple(zip(*batch))) for x in ['train', 'val']}\n"],"metadata":{"id":"s0imsehBao4R","executionInfo":{"status":"ok","timestamp":1695283623754,"user_tz":-180,"elapsed":265,"user":{"displayName":"Fedor Konovalenko","userId":"16758494924099257201"}}},"id":"s0imsehBao4R","execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["## Загрузка и кастомизация модели\n","❗❗❗ **Можно пробовать любые модели, которые понимают вот такой формат данных на входе:**\n","\n","During training, the model expects both the input tensors and a targets (list of dictionary), containing:\n","```\n","        boxes (FloatTensor[N, 4]): the ground-truth boxes in [x1, y1, x2, y2] format, with 0 <= x1 < x2 <= W and 0 <= y1 < y2 <= H.\n","\n","        labels (Int64Tensor[N]): the class label for each ground-truth box\n","\n","```"],"metadata":{"id":"ckbJz9dux4RB"},"id":"ckbJz9dux4RB"},{"cell_type":"code","execution_count":10,"metadata":{"id":"uC3YWvC276oc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1695283780942,"user_tz":-180,"elapsed":8612,"user":{"displayName":"Fedor Konovalenko","userId":"16758494924099257201"}},"outputId":"893e4336-bed7-4cc7-9a80-9d62493c8e1c"},"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/ssd300_vgg16_coco-b556d3b4.pth\" to /root/.cache/torch/hub/checkpoints/ssd300_vgg16_coco-b556d3b4.pth\n","100%|██████████| 136M/136M [00:00<00:00, 276MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["SSD(\n","  (backbone): SSDFeatureExtractorVGG(\n","    (features): Sequential(\n","      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): ReLU(inplace=True)\n","      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (3): ReLU(inplace=True)\n","      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (6): ReLU(inplace=True)\n","      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (8): ReLU(inplace=True)\n","      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (11): ReLU(inplace=True)\n","      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (13): ReLU(inplace=True)\n","      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (15): ReLU(inplace=True)\n","      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n","      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (18): ReLU(inplace=True)\n","      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (20): ReLU(inplace=True)\n","      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (22): ReLU(inplace=True)\n","    )\n","    (extra): ModuleList(\n","      (0): Sequential(\n","        (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","        (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (2): ReLU(inplace=True)\n","        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (4): ReLU(inplace=True)\n","        (5): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (6): ReLU(inplace=True)\n","        (7): Sequential(\n","          (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n","          (1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6))\n","          (2): ReLU(inplace=True)\n","          (3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n","          (4): ReLU(inplace=True)\n","        )\n","      )\n","      (1): Sequential(\n","        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n","        (1): ReLU(inplace=True)\n","        (2): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","        (3): ReLU(inplace=True)\n","      )\n","      (2): Sequential(\n","        (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n","        (1): ReLU(inplace=True)\n","        (2): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","        (3): ReLU(inplace=True)\n","      )\n","      (3-4): 2 x Sequential(\n","        (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n","        (1): ReLU(inplace=True)\n","        (2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n","        (3): ReLU(inplace=True)\n","      )\n","    )\n","  )\n","  (anchor_generator): DefaultBoxGenerator(aspect_ratios=[[2], [2, 3], [2, 3], [2, 3], [2], [2]], clip=True, scales=[0.07, 0.15, 0.33, 0.51, 0.69, 0.87, 1.05], steps=[8, 16, 32, 64, 100, 300])\n","  (head): SSDHead(\n","    (classification_head): SSDClassificationHead(\n","      (module_list): ModuleList(\n","        (0): Conv2d(512, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): Conv2d(1024, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (2): Conv2d(512, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (3): Conv2d(256, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (4-5): 2 x Conv2d(256, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","    )\n","    (regression_head): SSDRegressionHead(\n","      (module_list): ModuleList(\n","        (0): Conv2d(512, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): Conv2d(1024, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (2): Conv2d(512, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (3): Conv2d(256, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (4-5): 2 x Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","    )\n","  )\n","  (transform): GeneralizedRCNNTransform(\n","      Normalize(mean=[0.48235, 0.45882, 0.40784], std=[0.00392156862745098, 0.00392156862745098, 0.00392156862745098])\n","      Resize(min_size=(512,), max_size=512, mode='bilinear')\n","  )\n",")\n"]}],"source":["#import torchvision\n","#from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","\n","# load a model pre-trained on COCO\n","#model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n","\n","# replace the classifier with a new one, that has\n","# num_classes which is user-defined\n","#num_classes = 3  # 1 class (person) + background\n","# get number of input features for the classifier\n","#in_features = model.roi_heads.box_predictor.cls_score.in_features\n","## replace the pre-trained head with a new one\n","#model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","\n","def create_model(num_classes=91, size=512):\n","    # Load the Torchvision pretrained model.\n","    model = models.get_model(\"ssd300_vgg16\", weights=SSD300_VGG16_Weights.COCO_V1)\n","    # Retrieve the list of input channels.\n","    in_channels = _utils.retrieve_out_channels(model.backbone, (size, size))\n","    # List containing number of anchors based on aspect ratios.\n","    num_anchors = model.anchor_generator.num_anchors_per_location()\n","    # The classification head.\n","    model.head.classification_head = SSDClassificationHead(\n","        in_channels=in_channels,\n","        num_anchors=num_anchors,\n","        num_classes=num_classes,\n","    )\n","    # Image size for transforms.\n","    model.transform.min_size = (size,)\n","    model.transform.max_size = size\n","    return model\n","\n","model = create_model(3, 512)\n","print(model)"],"id":"uC3YWvC276oc"},{"cell_type":"markdown","source":["## Вспомогательные классы для осреденения результата и сохранения лучшей модели"],"metadata":{"id":"XmZYyB5OptC3"},"id":"XmZYyB5OptC3"},{"cell_type":"code","source":["class Averager:\n","    def __init__(self):\n","      self.current_total = 0.0\n","      self.iterations = 0.0\n","\n","    def send(self, value):\n","      self.current_total += value\n","      self.iterations += 1\n","\n","    @property\n","    def value(self):\n","      if self.iterations == 0:\n","        return 0\n","      else:\n","        return 1.0 * self.current_total / self.iterations\n","\n","    def reset(self):\n","      self.current_total = 0.0\n","      self.iterations = 0.0\n","\n","class SaveBestModel:\n","    \"\"\"\n","    Class to save the best model while training. If the current epoch's\n","    validation loss is less than the previous least less, then save the\n","    model state.\n","    \"\"\"\n","    def __init__(\n","        self, best_valid_loss=float('inf')\n","    ):\n","        self.best_valid_loss = best_valid_loss\n","\n","    def __call__(\n","        self, current_valid_loss,\n","        epoch, model, optimizer\n","    ):\n","      if current_valid_loss < self.best_valid_loss:\n","        self.best_valid_loss = current_valid_loss\n","        print(f\"\\nBest validation loss: {self.best_valid_loss}\")\n","        print(f\"\\nSaving best model for epoch: {epoch+1}\\n\")\n","        torch.save(model.state_dict(), modelDir + 'best_ssd300-3_model.pth')"],"metadata":{"id":"3fGpVajOTRph","executionInfo":{"status":"ok","timestamp":1695283851007,"user_tz":-180,"elapsed":332,"user":{"displayName":"Fedor Konovalenko","userId":"16758494924099257201"}}},"id":"3fGpVajOTRph","execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["## Функции для обучения и валидации"],"metadata":{"id":"9a_8V7EqyiU_"},"id":"9a_8V7EqyiU_"},{"cell_type":"code","source":["def train(train_data_loader, model):\n","  model.train()\n","  print('Training')\n","  global train_itr\n","  global train_loss_list\n","\n","     # initialize tqdm progress bar\n","  prog_bar = tqdm(train_data_loader, total=len(train_data_loader))\n","\n","  for i, data in enumerate(prog_bar):\n","    optimizer.zero_grad()\n","    images, targets = data\n","\n","    images = list(image.to(device) for image in images)\n","    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n","    loss_dict = model(images, targets)\n","    losses = sum(loss for loss in loss_dict.values())\n","    loss_value = losses.item()\n","    train_loss_list.append(loss_value)\n","    train_loss_hist.send(loss_value)\n","    losses.backward()\n","    optimizer.step()\n","    train_itr += 1\n","\n","        # update the loss value beside the progress bar for each iteration\n","    prog_bar.set_description(desc=f\"Loss: {loss_value:.4f}\")\n","  return train_loss_list"],"metadata":{"id":"MtovNXR4kM__","executionInfo":{"status":"ok","timestamp":1695283845177,"user_tz":-180,"elapsed":315,"user":{"displayName":"Fedor Konovalenko","userId":"16758494924099257201"}}},"id":"MtovNXR4kM__","execution_count":11,"outputs":[]},{"cell_type":"code","source":["def validate(valid_data_loader, model):\n","  print('Validating')\n","  global val_itr\n","  global val_loss_list\n","  #global map_list\n","\n","    # initialize tqdm progress bar\n","  prog_bar = tqdm(valid_data_loader, total=len(valid_data_loader))\n","\n","  for i, data in enumerate(prog_bar):\n","    images, targets = data\n","\n","    images = list(image.to(device) for image in images)\n","    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n","\n","    with torch.no_grad():\n","      model.train()\n","      loss_dict = model(images, targets)\n","      metric = MeanAveragePrecision(iou_type=\"bbox\")\n","      model.eval()\n","      output = model(images, targets)\n","      metric.update(output, targets)\n","      map_50 = metric.compute()['map_50']\n","\n","    losses = sum(loss for loss in loss_dict.values())\n","    loss_value = losses.item()\n","    val_loss_list.append(loss_value)\n","    #map_list.append(map_50)\n","    #map_hist.send(map_50)\n","    val_loss_hist.send(loss_value)\n","    val_itr += 1\n","    # update the loss value beside the progress bar for each iteration\n","    prog_bar.set_description(desc=f\"Loss: {loss_value:.4f}---mAP50: {map_50:.4f}\")\n","  return val_loss_list"],"metadata":{"id":"GbApxMkmRjKn","executionInfo":{"status":"ok","timestamp":1695283847095,"user_tz":-180,"elapsed":498,"user":{"displayName":"Fedor Konovalenko","userId":"16758494924099257201"}}},"id":"GbApxMkmRjKn","execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["## Обучение"],"metadata":{"id":"-0onfzGAynK-"},"id":"-0onfzGAynK-"},{"cell_type":"code","source":["optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n","#exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n","model = model.to(device)\n","train_loss_hist = Averager()\n","val_loss_hist = Averager()\n","#map_hist = Averager()\n","train_itr = 1\n","val_itr = 1\n","# train and validation loss lists to store loss values of all...\n","# ... iterations till ena and plot graphs for all iterations\n","train_loss_list = []\n","val_loss_list = []\n","#map_list = []\n","save_best_model = SaveBestModel()\n","# name to save the trained model with\n","NUM_EPOCHS = 30\n","# start the training epochs\n","for epoch in range(NUM_EPOCHS):\n","  print(f\"\\nEPOCH {epoch+1} of {NUM_EPOCHS}\")\n","  # reset the training and validation loss histories for the current epoch\n","  train_loss_hist.reset()\n","  val_loss_hist.reset()\n","  # start timer and carry out training and validation\n","  start = time.time()\n","  train_loss = train(dataloaders['train'], model)\n","  val_loss = validate(dataloaders['val'], model)\n","  print(f\"Epoch #{epoch+1} train loss: {train_loss_hist.value:.3f}\")\n","  print(f\"Epoch #{epoch+1} validation loss: {val_loss_hist.value:.3f}\")\n","  end = time.time()\n","  print(f\"Took {((end - start) / 60):.3f} minutes for epoch {epoch+1}\")\n","  # save the best model till now if we have the least loss in the...\n","  # ... current epoch\n","  save_best_model(\n","            val_loss_hist.value, epoch, model, optimizer\n","        )"],"metadata":{"id":"kgVNXRSpSSA4"},"id":"kgVNXRSpSSA4","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Загрузка весов лучшей из моделей"],"metadata":{"id":"xbzpFb3wy3qv"},"id":"xbzpFb3wy3qv"},{"cell_type":"code","source":["best_model_params_path = os.path.join(modelDir, 'best_ssd300-2_model.pth')\n","trained_model = model\n","trained_model.load_state_dict(torch.load(best_model_params_path))"],"metadata":{"id":"QmYRsZL12nii","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1695253524943,"user_tz":-180,"elapsed":246,"user":{"displayName":"Fedor Konovalenko","userId":"16758494924099257201"}},"outputId":"9918fb8d-68e1-4a7f-e401-9298ddbe5c9f"},"id":"QmYRsZL12nii","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":50}]},{"cell_type":"markdown","source":["## Функция для рисования картинок и границ объектов"],"metadata":{"id":"YmdiQBx0y9xn"},"id":"YmdiQBx0y9xn"},{"cell_type":"code","source":["def draw(path, model):\n","  image = read_image(path).to(torch.float32)\n","  with torch.no_grad():\n","    x = image.to(device)\n","    predictions = model.eval()([x, ])\n","  pred = predictions[0]\n","  image = (255.0 * (image - image.min()) / (image.max() - image.min())).to(torch.uint8)\n","  #pred_score = [f\"value: {score:.3f}\" for score in pred[\"scores\"]]\n","  #pred_label = [f\"value: {label}\" for label in pred[\"labels\"]]\n","  pred_boxes = pred[\"boxes\"].long()\n","  output_image = draw_bounding_boxes(image, pred_boxes, fill=True, width=3)\n","\n","  plt.figure(figsize=(12, 12))\n","  plt.imshow(output_image.permute(1, 2, 0))\n","  plt.show()"],"metadata":{"id":"PSMj3m75O1a1"},"id":"PSMj3m75O1a1","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Проверка на тестовых изображениях"],"metadata":{"id":"d7RU-l4ZzEy3"},"id":"d7RU-l4ZzEy3"},{"cell_type":"code","source":["testDir = 'drive/MyDrive/Colab Notebooks/samolet/data/test/'\n","tests = []\n","for filename in os.listdir(testDir):\n","  tests.append(filename)\n","\n","for i in np.random.randint(0, len(tests), 3):\n","  draw(testDir + tests[i], trained_model)"],"metadata":{"id":"ZcgPBflaOQGz"},"id":"ZcgPBflaOQGz","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"_4O5U77TbFc4"},"id":"_4O5U77TbFc4","execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":5}